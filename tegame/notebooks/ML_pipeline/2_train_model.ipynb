{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d266c6b2-797f-43a6-9867-315a9651765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "from dataset import TegameDataset\n",
    "from models import TegameScoreModel\n",
    " \n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d282d3f1-3a16-4747-b8d2-ea66db1e6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = torch.load(\"training_set.pt\")\n",
    "dataset = TegameDataset(groups)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25771adf-8dd9-46d8-bc12-90434ac6ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first group from the dataset\n",
    "X_group, y_group = dataset[0]\n",
    "\n",
    "# The feature dimension is the second dimension of X_group\n",
    "INPUT_DIM = X_group.shape[1]\n",
    "\n",
    "# 2. Model instance\n",
    "# hidden_dim=128 is a reasonable default: large enough to capture patterns,\n",
    "# small enough to avoid overfitting on limited gameplay data.\n",
    "model = TegameScoreModel(input_dim=INPUT_DIM, hidden_dim=128)\n",
    "\n",
    "# 3. Device selection\n",
    "# Use GPU if available; otherwise fall back to CPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 4. Optimizer\n",
    "# Adam is chosen because it converges faster and more stably than SGD\n",
    "# for this kind of supervised policy-learning setup.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754db4bd-eb39-437e-88fc-8100f2e3db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listwise_softmax_loss(logits, target_index):\n",
    "    \"\"\"\n",
    "    Computes a listwise softmax loss for a single decision group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : tensor of shape [num_moves]\n",
    "        The unnormalized scores assigned by the model to each possible move.\n",
    "\n",
    "    target_index : int\n",
    "        Index of the move actually taken (the correct action).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This loss corresponds to the negative log-likelihood of the chosen action\n",
    "    under a softmax distribution over all legal moves. It is the standard\n",
    "    objective for training a policy model in a listwise classification setting,\n",
    "    where each group has a variable number of actions.\n",
    "    \"\"\"\n",
    "    log_probs = torch.log_softmax(logits, dim=0)\n",
    "    return -log_probs[target_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939b1be7-11d0-4fc8-8737-bc529cb22252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 4127.9447\n",
      "Epoch 2/20 - Loss: 3044.9432\n",
      "Epoch 3/20 - Loss: 2825.9132\n",
      "Epoch 4/20 - Loss: 2712.5898\n",
      "Epoch 5/20 - Loss: 2819.7274\n",
      "Epoch 6/20 - Loss: 2664.1553\n",
      "Epoch 7/20 - Loss: 2611.4773\n",
      "Epoch 8/20 - Loss: 2667.2960\n",
      "Epoch 9/20 - Loss: 2507.7724\n",
      "Epoch 10/20 - Loss: 2495.1049\n",
      "Epoch 11/20 - Loss: 2490.9058\n",
      "Epoch 12/20 - Loss: 2688.3812\n",
      "Epoch 13/20 - Loss: 2597.6471\n",
      "Epoch 14/20 - Loss: 2478.3551\n",
      "Epoch 15/20 - Loss: 2411.5257\n",
      "Epoch 16/20 - Loss: 2483.6621\n",
      "Epoch 17/20 - Loss: 2428.2955\n",
      "Epoch 18/20 - Loss: 2430.6318\n",
      "Epoch 19/20 - Loss: 2473.6905\n",
      "Epoch 20/20 - Loss: 2513.8300\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X_group, y_group in loader:\n",
    "        # X_group: [1, num_moves, feature_dim]\n",
    "        # y_group: [1]\n",
    "\n",
    "        # Remove the batch dimension (always 1) and move to device\n",
    "        X_group = X_group.squeeze(0).to(device)   # [num_moves, feature_dim]\n",
    "        y_group = y_group.item()                  # int (index of the chosen move)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(X_group).squeeze(-1)       # [num_moves]\n",
    "\n",
    "        # Skip groups with only one possible move (typically only NOOP)\n",
    "        # These provide no learning signal, since the model has no choice to make.\n",
    "        if logits.ndim == 0:\n",
    "            continue\n",
    "\n",
    "        # Loss: negative log-likelihood of the chosen move\n",
    "        # Using log_softmax ensures numerical stability.\n",
    "        log_probs = torch.log_softmax(logits, dim=0)\n",
    "        loss = -log_probs[y_group]\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7e7be-de9c-4701-b406-b90574a9212e",
   "metadata": {},
   "source": [
    "> **Note:** The loss appears very large because it is the *sum* of the negative log‑likelihood over thousands of decision groups, not an average. Its absolute value is therefore not meaningful on its own — what matters is that it decreases over epochs.  \n",
    "> Now that the model is trained, we can evaluate something more interpretable: its accuracy in reproducing the moves from the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9eb288-3d0e-4b55-8af4-eb34215ea007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group accuracy: 77.2%\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_groups = 0\n",
    "total_groups = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_group, y_group in dataset:\n",
    "        # X_group: [num_moves, feature_dim]\n",
    "        # y_group: int (index of the correct move)\n",
    "\n",
    "        logits = model(X_group).squeeze(-1)   # [num_moves]\n",
    "\n",
    "        # Skip groups with only one possible move (typically NOOP)\n",
    "        # These provide no meaningful accuracy signal.\n",
    "        if logits.ndim == 0:\n",
    "            continue\n",
    "\n",
    "        pred_idx = logits.argmax().item()\n",
    "\n",
    "        if pred_idx == y_group:\n",
    "            correct_groups += 1\n",
    "\n",
    "        total_groups += 1\n",
    "\n",
    "# This accuracy measures how often the model reproduces\n",
    "# the same decisions found in the training set.\n",
    "acc = correct_groups / total_groups\n",
    "print(f\"Group accuracy: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4141c1-b316-4f89-9757-011ec3a72782",
   "metadata": {},
   "source": [
    "> **Comment:** Accuracy is a much more interpretable metric than the raw loss.  \n",
    "> While the loss only tells us that the model is assigning higher probability to the correct moves over time, the accuracy directly measures how often the model reproduces the same decisions found in the training logs.  \n",
    "> This gives us a clearer sense of how well the learned policy matches the behavior encoded in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0c800c-d9aa-418a-aab0-e4ea52eb6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "torch.save(model, \"tegame_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a01b8-9dfd-4ab6-84a8-25f24da50f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
