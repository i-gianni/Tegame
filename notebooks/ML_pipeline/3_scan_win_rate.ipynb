{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878e6d1c-3d45-4e0c-bb3d-7461807f551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "from tegame import Tegame,TegameML\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8147ee85-738e-42aa-8373-4720c09a7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of simulations\n",
    "N_sim = 100\n",
    "\n",
    "# 2. Generate many independent initial game states\n",
    "# These states will be used to evaluate the model across multiple\n",
    "# randomized starting conditions, ensuring that the analysis is not\n",
    "# biased by a single initial configuration.\n",
    "states = []\n",
    "for _ in range(N_sim):\n",
    "    game = Tegame(players=2)\n",
    "    game.restart()\n",
    "    states.append(game.get_state().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae933293-946e-45b5-a521-99476da8ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 318.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Rate: 2.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_code = []\n",
    "\n",
    "for i in tqdm(range(N_sim)):\n",
    "    game = Tegame(players=2, verb_lvl=0,\n",
    "                  thresh_nonmandatory=2,\n",
    "                  thresh_secondchoice=4)\n",
    "    game.restart()\n",
    "\n",
    "    # Set the sampled initial state\n",
    "    # This allows us to evaluate the model starting from the same\n",
    "    # randomized initial conditions generated earlier.\n",
    "    state = deepcopy(states[i])\n",
    "    game.set_state(state)\n",
    "\n",
    "    game_won = game.run_game()  # must return True (win) or False (loss)\n",
    "    results_code.append(game_won)\n",
    "\n",
    "print(f\"Win Rate: {100 * sum(results_code) / len(results_code):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97c77e4-df36-476e-9f27-59fe0b5436bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full saved model (architecture + weights).\n",
    "# weights_only=False is required because we want to restore the entire\n",
    "# model object, not just its parameter tensors.\n",
    "model = torch.load(\"tegame_model.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72502b1d-e310-4477-bcdf-d9bdc2e919b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Rate: 1.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_ML = []\n",
    "\n",
    "wins = 0\n",
    "losses = 0\n",
    "\n",
    "for i in tqdm(range(N_sim)):\n",
    "    # Run the ML-based agent instead of the heuristic one\n",
    "    game = TegameML(model, players=2, verb_lvl=0)\n",
    "    game.restart()\n",
    "\n",
    "    # Set the sampled initial state\n",
    "    # This ensures we evaluate the ML model starting from the same\n",
    "    # initial conditions used in the baseline simulations.\n",
    "    state = deepcopy(states[i])\n",
    "    game.set_state(state)\n",
    "\n",
    "    game_won = game.run_game()  # must return True (win) or False (loss)\n",
    "    results_ML.append(game_won)\n",
    "\n",
    "print(f\"Win Rate: {100 * sum(results_ML) / len(results_ML):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf0934-eca2-4c06-8e6d-323ccf8791e5",
   "metadata": {},
   "source": [
    "> **Comment:** The model performs quite well, but it’s expected that its win rate is still lower than the hand‑crafted code. After all, the model is only learning to imitate the moves seen in the dataset — it’s not a reinforcement‑learning agent (at least not yet)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
